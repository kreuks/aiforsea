{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('models/result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>changes</th>\n",
       "      <th>average_roc_auc_xgboost</th>\n",
       "      <th>average_roc_auc_lightgbm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>add more features</td>\n",
       "      <td>0.757521</td>\n",
       "      <td>0.757234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>with log1p</td>\n",
       "      <td>0.755344</td>\n",
       "      <td>0.757432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>random under sampling 1:1</td>\n",
       "      <td>0.744590</td>\n",
       "      <td>0.741374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>random under sampling 1:2</td>\n",
       "      <td>0.748584</td>\n",
       "      <td>0.748759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>top 50 feature from exp id 1 model</td>\n",
       "      <td>0.763203</td>\n",
       "      <td>0.756852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>top 100 feature from exp id 1 model</td>\n",
       "      <td>0.763870</td>\n",
       "      <td>0.755584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>top 150 feature from exp id 1 model</td>\n",
       "      <td>0.758344</td>\n",
       "      <td>0.757426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>top 200 feature from exp id 1 model</td>\n",
       "      <td>0.762214</td>\n",
       "      <td>0.757075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id                              changes  \\\n",
       "0              1                    add more features   \n",
       "1              2                           with log1p   \n",
       "2              3            random under sampling 1:1   \n",
       "3              4            random under sampling 1:2   \n",
       "4              5   top 50 feature from exp id 1 model   \n",
       "5              6  top 100 feature from exp id 1 model   \n",
       "6              7  top 150 feature from exp id 1 model   \n",
       "7              8  top 200 feature from exp id 1 model   \n",
       "\n",
       "   average_roc_auc_xgboost  average_roc_auc_lightgbm  \n",
       "0                 0.757521                  0.757234  \n",
       "1                 0.755344                  0.757432  \n",
       "2                 0.744590                  0.741374  \n",
       "3                 0.748584                  0.748759  \n",
       "4                 0.763203                  0.756852  \n",
       "5                 0.763870                  0.755584  \n",
       "6                 0.758344                  0.757426  \n",
       "7                 0.762214                  0.757075  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "Experiment using 5 fold cross validation. <br>\n",
    "Here ilustration using 5 fold cross validation\n",
    "<br><img src=\"https://www.researchgate.net/profile/Mariia_Fedotenkova/publication/311668395/figure/fig5/AS:613923871019041@1523382265447/A-schematic-illustration-of-K-fold-cross-validation-for-K-5-Original-dataset-shown.png\">\n",
    "<br>\n",
    "<br>\n",
    "I count roc auc metrics for every fold and then after all 5 folds roc auc metric has been calculated, I calculate average roc auc for all 5 folds. Hyperparameter tuning was done using hyperopt (bayesian optimization) library. <br><br>\n",
    "From analysis before, I assume that log1p transformation will boost performance because it will transform skewed distribution to normal distribution. It turns out, log1p in xgboost will reduce overall performance and log1p in lightgbm just slightly better (not too significant). <br><br>\n",
    "In first analysis, it turns out that this data has imbalanced class problem although this data has 1:3 ratio (1 positive : 3 negative). So I initiative to experiment to undersampling the training data (for every fold I resample the training data), but the result is it can degrade overall performance and then I just leave training data as is withouth resampling. <br><br>\n",
    "Last 4 experiment, I try to reduce number of feature using feature importance and the best result is use top 100 features from xgboost. So this method will be my final model. <br><br>\n",
    "The best model is using xgboost algorithm and use only top 100 features from feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
